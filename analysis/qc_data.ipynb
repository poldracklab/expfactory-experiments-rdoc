{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"ax_cpt_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"cued_task_switching_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"flanker_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"go_nogo_rdoc\": {\n",
    "        \"accuracy\": 0.6,\n",
    "        \"rt\": 1000,\n",
    "        \"omissions\": 0.2,\n",
    "        \"check_response\": 0.6,\n",
    "    },\n",
    "    \"n_back_rdoc\": {\n",
    "        \"accuracy\": 0.6,\n",
    "        \"rt\": 1000,\n",
    "        \"omissions\": 0.2,\n",
    "        \"check_response\": 0.6,\n",
    "    },\n",
    "    \"span_rdoc__behavioral\": {\"accuracy\": 0.25, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"spatial_cueing_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"spatial_task_switching_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"stop_signal_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"stroop_rdoc\": {\"accuracy\": 0.6, \"rt\": 1000, \"omissions\": 0.2},\n",
    "    \"visual_search_rdoc\": {\"accuracy\": 0.6, \"rt\": 1500, \"omissions\": 0.2},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_generator(\n",
    "    task, attention_check_accuracy, accuracy, rt, omissions, check_response=None\n",
    "):\n",
    "    feedbacks = []\n",
    "    threshold = thresholds[task]\n",
    "\n",
    "    if attention_check_accuracy < 0.6:\n",
    "        feedback = f\"Overall attention check accuracy of {attention_check_accuracy*100:.2f}% is low for {task}.\"\n",
    "        feedbacks.append(feedback)\n",
    "\n",
    "    if accuracy < threshold['accuracy']:\n",
    "        feedback = f\"Overall task accuracy of {accuracy*100:.2f}% is low for {task}.\"\n",
    "        feedbacks.append(feedback)\n",
    "    if rt > threshold['rt']:\n",
    "        feedback = f\"Overall rt of {rt} is high for {task}.\"\n",
    "        feedbacks.append(feedback)\n",
    "    if omissions > threshold['omissions']:\n",
    "        feedback = f\"Overall omissions of {omissions*100:.2f}% is high for {task}.\"\n",
    "        feedbacks.append(feedback)\n",
    "\n",
    "    if check_response != None:\n",
    "        if check_response > threshold['check_response']:\n",
    "            feedback = f\"Single response proportion of {check_response} is high for {task}.\"\n",
    "            feedbacks.append(feedback)\n",
    "\n",
    "    return feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(file):\n",
    "    with open(file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for task, task_data in data.items():\n",
    "        for task_item in task_data:\n",
    "            sub_id = task_item[\"subject\"]\n",
    "            dict_obj = ast.literal_eval(task_item[\"data\"])\n",
    "\n",
    "            # Check if 'trialdata' exists in dict_obj\n",
    "            if \"trialdata\" not in dict_obj:\n",
    "                continue  # Skip to the next iteration if 'trialdata' is not present\n",
    "\n",
    "            # Check the type of 'trialdata'\n",
    "            if isinstance(dict_obj[\"trialdata\"], str):\n",
    "                trial_data = json.loads(dict_obj[\"trialdata\"])\n",
    "            else:\n",
    "                trial_data = dict_obj[\"trialdata\"]\n",
    "\n",
    "            single_sub_df = pd.DataFrame(trial_data)\n",
    "\n",
    "            if sub_id not in data_dict:\n",
    "                data_dict[sub_id] = {}\n",
    "\n",
    "            if task not in data_dict[sub_id]:\n",
    "                data_dict[sub_id][task] = []\n",
    "\n",
    "            data_dict[sub_id][task].append(single_sub_df)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './data/subject_preview_111_2024_01_29_23_28_12_2024.01.30.022027.json'\n",
    "data_dict = organize_data(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_check_accuracy(df):\n",
    "    attention_checks = df[(df[\"trial_id\"] == \"test_attention_check\")]\n",
    "    attention_check_accuracy = attention_checks[\"correct_trial\"].mean()\n",
    "    return attention_check_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_processing_rt(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_inter-stimulus\"]\n",
    "    correct_test_trials = test_trials[test_trials[\"correct_trial\"] == 1]\n",
    "    average_rt = correct_test_trials[\"rt\"].mean()\n",
    "    return average_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_omissions(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    proportion = (test_trials[\"response\"].apply(lambda x: len(x) == 4)).mean()\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_rt(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    correct_test_trials = test_trials[test_trials[\"correct_trial\"] == 1]\n",
    "    average_rt = correct_test_trials['rt'].mean()\n",
    "    return average_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    average_accuracy = test_trials['correct_trial'].mean()\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_accuracy(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "\n",
    "    responses = test_trials[\"response\"]\n",
    "    correct_spatial_sequences = test_trials[\"spatial_sequence\"]\n",
    "\n",
    "    # Function to calculate the accuracy for each row\n",
    "    def calculate_accuracy(response, correct_sequence):\n",
    "        correct_responses = len(set(response).intersection(correct_sequence))\n",
    "        return correct_responses / len(correct_sequence)\n",
    "\n",
    "    # Calculating accuracies for each row\n",
    "    accuracies = [\n",
    "        calculate_accuracy(resp, corr_seq)\n",
    "        for resp, corr_seq in zip(responses, correct_spatial_sequences)\n",
    "    ]\n",
    "\n",
    "    # Calculating the mean accuracy for the whole dataframe\n",
    "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_omissions(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    average_omissions = test_trials[\"rt\"].isna().mean()\n",
    "    return average_omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_n_back_responses(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    value_counts = test_trials[\"response\"].value_counts()\n",
    "\n",
    "    proportions = value_counts / test_trials[\"response\"].count()\n",
    "    mismatch_correct_response = df[\n",
    "            (df[\"correct_trial\"] == 1) & (df[\"condition\"] == \"mismatch\")\n",
    "        ][\"response\"].unique()[0]\n",
    "\n",
    "    return proportions[mismatch_correct_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_go_nogo_responses(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    value_counts = test_trials[\"response\"].value_counts()\n",
    "\n",
    "    proportions = value_counts / test_trials[\"response\"].count()\n",
    "    mismatch_correct_response = df[\n",
    "            (df[\"correct_trial\"] == 1) & (df[\"condition\"] == \"go\")\n",
    "        ][\"response\"].unique()[0]\n",
    "\n",
    "    return proportions[mismatch_correct_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopping(df):\n",
    "    test_trials = df[df[\"trial_id\"] == \"test_trial\"]\n",
    "    go_trials = test_trials[test_trials[\"condition\"] == \"go\"]\n",
    "    correct_go_trials = go_trials[go_trials[\"correct_trial\"] == 1]\n",
    "    stop_trials = test_trials[test_trials[\"condition\"] == \"stop\"]\n",
    "\n",
    "    go_accuracy = go_trials[\"correct_trial\"].mean()\n",
    "    go_omissions = go_trials[\"rt\"].isna().mean()\n",
    "    stop_accuracy = stop_trials[\"correct_trial\"].mean()\n",
    "    go_rt = correct_go_trials[\"rt\"].mean()\n",
    "\n",
    "    max_SSD = test_trials[\"SSD\"].max()\n",
    "    min_SSD = test_trials[\"SSD\"].min()\n",
    "    mean_SSD = test_trials[\"SSD\"].mean()\n",
    "    SSD = test_trials[\"SSD\"].iloc[-1]\n",
    "\n",
    "    return {\n",
    "        'go_accuracy': go_accuracy, \n",
    "        'go_omissions': go_omissions, \n",
    "        'stop_accuracy': stop_accuracy, \n",
    "        'go_rt': go_rt, \n",
    "        'max_SSD': max_SSD, \n",
    "        'min_SSD':  min_SSD, \n",
    "        'mean_SSD': mean_SSD,\n",
    "        'SSD': SSD\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in data_dict:\n",
    "    for task in data_dict[sub]:\n",
    "        array_of_task_dfs = data_dict[sub][task]\n",
    "        for task_df in array_of_task_dfs:\n",
    "            print(task)\n",
    "            attention_check_accuracy = get_attention_check_accuracy(task_df)\n",
    "            average_accuracy = get_accuracy(task_df)\n",
    "\n",
    "            if task == \"span_rdoc__behavioral\":\n",
    "                average_rt = get_span_processing_rt(task_df)\n",
    "                average_omissions = get_span_omissions(task_df)\n",
    "                average_accuracy = get_span_accuracy(task_df)\n",
    "            elif task == \"stop_signal_rdoc\":\n",
    "                stopping_data = get_stopping(task_df)\n",
    "            else:\n",
    "                average_rt = get_average_rt(task_df)\n",
    "                average_omissions = get_omissions(task_df)\n",
    "\n",
    "            if task == \"n_back_rdoc\":\n",
    "                check_response = check_n_back_responses(task_df)\n",
    "            elif task == \"go_nogo_rdoc\":\n",
    "                check_response = check_go_nogo_responses(task_df)\n",
    "            else:\n",
    "                check_response = None\n",
    "\n",
    "            feedback = feedback_generator(\n",
    "                task,\n",
    "                attention_check_accuracy,\n",
    "                average_accuracy,\n",
    "                average_rt,\n",
    "                average_omissions,\n",
    "                check_response,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
